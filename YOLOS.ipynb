{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d72bf0-08b1-4b0f-8ae7-cd2297081db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "import scipy\n",
    "import torchvision\n",
    "import os\n",
    "import wandb\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from transformers import DetrConfig, AutoModelForObjectDetection\n",
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13793030-4184-49cb-83f7-9931b6c8f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = Roboflow(api_key=\"your API key\")\n",
    "project = #insert roboflow workplace\n",
    "version = project.version(\"project version\")\n",
    "dataset = version.download(\"coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2701c5-bf2e-4cbf-b805-e6afb0ef1858",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1673139-11b0-4a56-b855-183776f255cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, feature_extractor, train=True):\n",
    "        ann_file = os.path.join(img_folder, \"_annotations.coco.json\")\n",
    "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read in PIL image and target in COCO format\n",
    "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "\n",
    "        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        encoding = self.feature_extractor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n",
    "        target = encoding[\"labels\"][0] # remove batch dimension\n",
    "\n",
    "        return pixel_values, target\n",
    "\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"hustvl/yolos-base\", size=660, max_size=660)\n",
    "\n",
    "train_dataset = CocoDetection(img_folder=(dataset.location + '/train'), feature_extractor=feature_extractor)\n",
    "val_dataset = CocoDetection(img_folder=(dataset.location + '/valid'), feature_extractor=feature_extractor, train=False)\n",
    "\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba60d43-9bb0-4df1-9a2f-e34c21331fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = train_dataset.coco.getImgIds()\n",
    "image_id = image_ids[np.random.randint(0, len(image_ids))]\n",
    "print('Image nÂ°{}'.format(image_id))\n",
    "\n",
    "image = train_dataset.coco.loadImgs(image_id)[0]\n",
    "image = Image.open(os.path.join(dataset.location + '/train', image['file_name']))\n",
    "\n",
    "annotations = train_dataset.coco.imgToAnns[image_id]\n",
    "draw = ImageDraw.Draw(image, \"RGBA\")\n",
    "\n",
    "cats = train_dataset.coco.cats\n",
    "id2label = {k: v['name'] for k,v in cats.items()}\n",
    "\n",
    "for annotation in annotations:\n",
    "  box = annotation['bbox']\n",
    "  class_idx = annotation['category_id']\n",
    "  x,y,w,h = tuple(box)\n",
    "  draw.rectangle((x,y,x+w,y+h), outline='red', width=1)\n",
    "  draw.text((x, y), id2label[class_idx], fill='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75848df7-f291-4399-a51f-9903f0f1bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  pixel_values = [item[0] for item in batch]\n",
    "  encoding = feature_extractor.pad(pixel_values, return_tensors=\"pt\")\n",
    "  labels = [item[1] for item in batch]\n",
    "  batch = {}\n",
    "  batch['pixel_values'] = encoding['pixel_values']\n",
    "  batch['labels'] = labels\n",
    "  return batch\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size= 4, shuffle=True, num_workers=8)\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=5, num_workers=5)\n",
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d477d0a-c61a-4284-901a-180200be76bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloS(pl.LightningModule):\n",
    "\n",
    "     def __init__(self, lr, weight_decay):\n",
    "         super().__init__()\n",
    "         self.model = AutoModelForObjectDetection.from_pretrained(\"hustvl/yolos-base\",\n",
    "                                                             num_labels=len(id2label),\n",
    "                                                             ignore_mismatched_sizes=True)\n",
    "         self.lr = lr\n",
    "         self.weight_decay = weight_decay\n",
    "         self.save_hyperparameters()  # adding this will save the hyperparameters to W&B too\n",
    "\n",
    "     def forward(self, pixel_values):\n",
    "       outputs = self.model(pixel_values=pixel_values)\n",
    "\n",
    "       return outputs\n",
    "\n",
    "     def common_step(self, batch, batch_idx):\n",
    "       pixel_values = batch[\"pixel_values\"]\n",
    "       labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "       outputs = self.model(pixel_values=pixel_values, labels=labels)\n",
    "\n",
    "       loss = outputs.loss\n",
    "       loss_dict = outputs.loss_dict\n",
    "\n",
    "       return loss, loss_dict\n",
    "\n",
    "     def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        self.log(\"train/loss\", loss)  \n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"train/\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        self.log(\"validation/loss\", loss) \n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"validation/\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr,\n",
    "                                  weight_decay=self.weight_decay)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "     def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "     def val_dataloader(self):\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72515da-a4e4-4fb5-9ee5-21addf01a43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YoloS(lr=2.1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ecf600-85e3-4310-aa16-baae230a340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "wandb_logger = WandbLogger(project='roboflow-yolos', log_model=True)\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"train/loss\", mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb668ee8-e33e-4b00-8b1c-a2dd60b1380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor= 'train/loss',\n",
    "    patience=5,\n",
    "    mode=\"min\",\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7336e6-bbc0-4b00-a070-02ba82258336",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(accelerator=\"gpu\", devices=1, max_epochs=300, gradient_clip_val=0.5, accumulate_grad_batches=1,\n",
    "                  log_every_n_steps=5, logger=wandb_logger, callbacks=[checkpoint_callback, early_stop_callback], precision=\"16-mixed\")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d7273-f1e8-4340-9bf7-f1a3e2f26383",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b5e2f7-71c9-4c35-ba3f-31812720a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/detr.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629a2ef-f8cb-4511-812e-958914b45cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"./detr\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96981582-c97a-4282-8e7e-94fb95a87f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_coco_api_from_dataset\n",
    "base_ds = get_coco_api_from_dataset(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96673b31-0538-41e1-9a1d-fefc9a625763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.coco_eval import CocoEvaluator\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "iou_types = ['bbox']\n",
    "coco_evaluator = CocoEvaluator(base_ds, iou_types) \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "\n",
    "for idx, batch in enumerate(tqdm(val_dataloader)):\n",
    "    pixel_values = batch[\"pixel_values\"].to(device)\n",
    "    labels = [{k: v.to(device) for k, v in t.items()} for t in batch[\"labels\"]] \n",
    "\n",
    "    outputs = model.model(pixel_values=pixel_values)\n",
    "\n",
    "    orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "    results = feature_extractor.post_process(outputs, orig_target_sizes) \n",
    "    res = {target['image_id'].item(): output for target, output in zip(labels, results)}\n",
    "    coco_evaluator.update(res)\n",
    "\n",
    "coco_evaluator.synchronize_between_processes()\n",
    "coco_evaluator.accumulate()\n",
    "coco_evaluator.summarize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
